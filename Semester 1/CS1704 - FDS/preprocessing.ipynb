{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482d9ffe",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d69fbb",
   "metadata": {},
   "source": [
    "In real-world scenarios, data is often messy and not immediately ready for use. Therefore, data preprocessing is essential for efficient model training.\n",
    "<br>\n",
    "\n",
    "Typical data preprocessing challenges:\n",
    "- Handling missing values\n",
    "- Scaling numerical features\n",
    "- Converting categorical attributes to numerical format\n",
    "- Reducing the number of features (dimensionality reduction)\n",
    "- Extracting features from non-numerical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4718fb",
   "metadata": {},
   "source": [
    "In Python, a library called `sklearn` (SciKit Learn) offers a wide variety of pre-processing transformers. It goes well with `numpy`, a library used for creating and processing numerical arrays.\n",
    "<br>\n",
    "\n",
    "It is important to use the same set of pre-processing for both the training data and the testing data. SKLearn offers a pipeline that makes it easy to chain multiple pre-processing transformers and apply them uniformly in the training data and the testing data. Here is a list of libraries provided by SKLearn for data pre-processing:\n",
    "- Feature Extraction\n",
    "- Data Cleaning\n",
    "- Feature Reduction\n",
    "- Feature Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e87d44",
   "metadata": {},
   "source": [
    "Consider the given datas (in the form of arrays) which are not clean and messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55dffcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Sample data with missing values\n",
    "X = [[1, 2], [np.nan, 3], [7, 6], [1, 4]]\n",
    "Y = [[np.nan, 2, 5], [6, np.nan, 4], [np.nan, 7, 6], [8, 2, np.nan]]\n",
    "Z = [[12], [2], [4], [-3], [6], [-5], [-100]]\n",
    "A = [[1], [2], [3], [9], [2], [3], [1]]\n",
    "B = [[1, \"human\", \"gamer\"], [2, \"robot\", \"machine\"], [0, \"bot\", \"speaker\"]]\n",
    "C = [[i] for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9d781",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b1360",
   "metadata": {},
   "source": [
    "### Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a77aa",
   "metadata": {},
   "source": [
    "Imputers are used to handle missing values in a dataset. The way imputers work is they rely on nearby data to the missing values and fill in the gap by doing some process with the nearby data.\n",
    "<br>\n",
    "\n",
    "Missing values include empty fields as well as non-numeric values (called NaN or Not a Number).\n",
    "<br>\n",
    "\n",
    "To use imputers, `impute` module from the `sklearn` library must be imported.\n",
    "<br>\n",
    "There are two main imputer APIs available in sklearn:\n",
    "- SimpleImputer\n",
    "- KNNImputer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad6571",
   "metadata": {},
   "source": [
    "#### SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b154f",
   "metadata": {},
   "source": [
    "This kind of imputer expects a 2D array, and fills in the missing values by checking the values in the column, and applies a strategy to compute a result with those values. The strategies available are:\n",
    "- mean: Computes the mean of other datas and fills in the result\n",
    "- median: Computes the median of other datas and fills in the result\n",
    "- most_frequent (Mode): Computes the mode of other datas and fills in the result\n",
    "- constant\n",
    "\n",
    "<br>\n",
    "\n",
    "If a column has multiple missing values, it fills in the values one by one. It ignores the other missing values in the column while processing.\n",
    "<br>\n",
    "\n",
    "To use this imputer, import `SimpleImputer` from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba2fb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 3.],\n",
       "       [7., 6.],\n",
       "       [1., 4.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filling in missing values with the mean strategy\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "imp.fit(X) #Learn the statistics from the data provided with variable X\n",
    "imp.transform(X) #Apply the learned stats to fill in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5efb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [1., 3.],\n",
       "       [7., 6.],\n",
       "       [1., 4.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filling in missing values with the median strategy\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "imp.fit(X)\n",
    "imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1705ab4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [1., 3.],\n",
       "       [7., 6.],\n",
       "       [1., 4.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filling in missing values with the most_frequent strategy\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "imp.fit(X)\n",
    "imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4373242e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [0., 3.],\n",
       "       [7., 6.],\n",
       "       [1., 4.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filling in missing values with the constant strategy\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='constant')\n",
    "\n",
    "imp.fit(X)\n",
    "imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045ef81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.        , 2.        , 5.        ],\n",
       "       [6.        , 3.66666667, 4.        ],\n",
       "       [7.        , 7.        , 6.        ],\n",
       "       [8.        , 2.        , 5.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filling in missing values for a dataset with multiple nan values in a column\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(Y)\n",
    "imp.transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1024755",
   "metadata": {},
   "source": [
    "#### KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4749d7a1",
   "metadata": {},
   "source": [
    "This kind of imputer expects a 2D array data, and the missing values are filled by computing the mean of `n` nearest neighbours, where `n` is a value provided by the user.\n",
    "\n",
    "<br>\n",
    "\n",
    "To compute the nearest neighbours, it used euclidean distance, which is calcuulated using the formula:\n",
    "$$\\text {dist} = \\sqrt {\\text {weight} \\cdot \\text {(distance between the coordinates)}^2}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The weight is calculated by using the formula:\n",
    "$$\\text {weight} = \\frac{\\text {total number of data}}{\\text {number of numeric data}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The distance between the coordinates is the difference between the value on the row which contains the missing value and the value on the row which we consider. Both the values must be on the same column. If a value is nan or missing, we ignore its distance.\n",
    "\n",
    "<br>\n",
    "\n",
    "Consider the example: `X = [[1, 2, 3], [3, np.nan, 6], [np.nan, 4, 5]]`\n",
    "<br>\n",
    "\n",
    "The second row contains a nan value. So we take the second row (say `a`), and we also choose a row to compare with, say the first row (say `b`). \n",
    "<br>\n",
    "\n",
    "Now, `a = [1, 2, 3]` and `b = [3, np.nan, 6]`\n",
    "<br>\n",
    "\n",
    "The second column has a nan value, and it will be ignored. So the distance becomes : $$\\text {dist} = (1 - 3)^2 + (3 - 6)^2 = 13$$\n",
    "<br>\n",
    "\n",
    "There are three values in a row and only two columns don't have nan or missing values. So weight : $$\\text {weight} = \\frac {3}{2}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Now the euclidian distance becomes : $$\\sqrt {\\frac {3}{2} \\cdot 13} = \\sqrt {\\frac {39}{2}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "To use this imputer, import `KNNImputer` from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0b943c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. , 4. ],\n",
       "       [3. , 4. , 3. ],\n",
       "       [5.5, 6. , 5. ],\n",
       "       [8. , 8. , 7. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filling in the missing values using KNNImputer\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imp = KNNImputer(n_neighbors=2)\n",
    "X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n",
    "imp.fit(X)\n",
    "imp.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673719f6",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d295d7a6",
   "metadata": {},
   "source": [
    "Scalers are used to scale numbers so all of them are on the same scale. When values are on different scales, convergeence of iterative optimization procedures becomes slower.\n",
    "<br>\n",
    "\n",
    "To use scalers, the `preprocessing` module from `sklearn` must be imported.\n",
    "<br>\n",
    "\n",
    "There are three main Scaler APIs available in sklearn:\n",
    "- StandardScaler\n",
    "- MinMaxScaler\n",
    "- MaxAbsoluteScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f0caa",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee64101",
   "metadata": {},
   "source": [
    "Transforms the original feature `x` into `x'` using the formula: \n",
    "$$x \\text {'} = \\frac{x - \\mu}{\\sigma} \\text {, where } \\mu \\text { = Mean; } \\sigma \\text { = Standard Deviation}$$\n",
    "<br>\n",
    "\n",
    "To use this scaler, import the `StandardScaler` class from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff8f8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.66107927],\n",
       "       [ 0.38562957],\n",
       "       [ 0.44071951],\n",
       "       [ 0.24790472],\n",
       "       [ 0.49580945],\n",
       "       [ 0.19281479],\n",
       "       [-2.42395731]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using StandardScaler to unify the scale\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(Z)\n",
    "ss.transform(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b25ee4",
   "metadata": {},
   "source": [
    "#### MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ed307",
   "metadata": {},
   "source": [
    "Transforms the original feature `x` into `x'` using the formula: \n",
    "$$x \\text {'} = \\frac{x - x_\\text {min}}{x_\\text {max} - x_\\text {min}}$$\n",
    "Where $x_\\text {min}$ is the minimum value in the matrix and $x_\\text {max}$ is the maximum value in the matrix. The scaled matrix has values from to 0 to 1, with both ends being present in any such matrix.\n",
    "<br>\n",
    "\n",
    "To use this scaler, import the `MinMaxScaler` class from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d7d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        ],\n",
       "       [0.91071429],\n",
       "       [0.92857143],\n",
       "       [0.86607143],\n",
       "       [0.94642857],\n",
       "       [0.84821429],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using MinMaxScaler to unify the scale\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "mms.fit_transform(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473547b",
   "metadata": {},
   "source": [
    "#### MaxAbsoluteScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d84ad7e",
   "metadata": {},
   "source": [
    "Transforms the original feature `x` into `x'` using the formula: \n",
    "$$x \\text {'} = \\frac{x}{\\text {MaxAbsoluteValue}}$$\n",
    "Where $\\text {MaxAbsoluteValue} = max \\lbrace x_{max}, |x_{min}| \\rbrace$. The scaled matrix has values from to -1 to 1, with -1 being present in any such matrix.\n",
    "<br>\n",
    "\n",
    "To use this scaler, import the `MinMaxScaler` class from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81501231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12],\n",
       "       [ 0.02],\n",
       "       [ 0.04],\n",
       "       [-0.03],\n",
       "       [ 0.06],\n",
       "       [-0.05],\n",
       "       [-1.  ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using MaxAbsoluteScaler to scale values\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "mas = MaxAbsScaler()\n",
    "mas.fit_transform(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f314d0",
   "metadata": {},
   "source": [
    "#### RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f420de",
   "metadata": {},
   "source": [
    "This scaler is specifically used in data to reduce the effects of outliers. It does so using Median and Inter Quartile Range (IQR) which do not get affected by outliers.\n",
    "<br>\n",
    "\n",
    "**Outlier**: Any value in a data that greatly impacts the balance of the data. It is on the lower extremes and upper extremes, away from the common range of the rest of the data values.\n",
    "<br>\n",
    "\n",
    "**Inter Quartile Range (IQR)**: The difference between the third qurtile Q3 (75 percentile) and first quartile (25 percentile).\n",
    "<br>\n",
    "\n",
    "To find them, First find the median of the data (which is the second quartile Q2 - 50 percentile), and split the data into two where the median lies between the highest of the first part and the lowest of the second. Then, the median of the first part becomes Q1 and median of second part becomes Q3.\n",
    "<br>\n",
    "\n",
    "This scaler transforms the original feature `x` to `x'` using the formula:\n",
    "$$x \\text ' = \\frac {x - Q_2}{Q_3 - Q_1}$$\n",
    "<br>\n",
    "\n",
    "To use this scaler, import the `RobustScaler` class from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79dbc091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.11111111],\n",
       "       [  0.        ],\n",
       "       [  0.22222222],\n",
       "       [ -0.55555556],\n",
       "       [  0.44444444],\n",
       "       [ -0.77777778],\n",
       "       [-11.33333333]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using RobustScaler to reduce the effect of outliers\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rs = RobustScaler()\n",
    "rs.fit_transform(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead21f8",
   "metadata": {},
   "source": [
    "#### Function Transformer\n",
    "Constructs transformed features using a user defined function.\n",
    "<br>\n",
    "\n",
    "To use it, import the `FunctionTransformer` class from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c281eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       ],\n",
       "       [1.       ],\n",
       "       [1.5849625],\n",
       "       [3.169925 ],\n",
       "       [1.       ],\n",
       "       [1.5849625],\n",
       "       [0.       ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Function Transformer\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "ft = FunctionTransformer(np.log2)\n",
    "ft.fit_transform(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649de21",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfb32d",
   "metadata": {},
   "source": [
    "Encoder is pre-processing tool that encodes a categorical dataset into numeric form, so that models can understand and work with it better. It is really important while training models, because they don't understand text or any other form of data while processing, other than numeric data.  \n",
    "<br>\n",
    "\n",
    "There are three main encoders:\n",
    "- OneHotEncoder\n",
    "- LabelEncoder\n",
    "- OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf2708",
   "metadata": {},
   "source": [
    "#### OneHotEncoder\n",
    "\n",
    "An encoding method that converts each categorical feature (arranged in ascending / alphabetical order) into a column with binary. Each row represents whether the data is present at the corresponding index or not. If the value in the row is 0, then that feature is not there in that particular index. If it is one, then it is.\n",
    "<br>\n",
    "\n",
    "The number of columns in the encoded matrix is equal to the number of unique features in the data.\n",
    "<br>\n",
    "\n",
    "To use this encoder, import `OneHotEncoder` from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "300831f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Appling OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit_transform(A).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf7133",
   "metadata": {},
   "source": [
    "#### LabelEncoder\n",
    "\n",
    "An encoding method that converts each categorical feature (arranged in increasing order) into its corresponding label. Assigning labels is done by first arranging the features in increasing order, and assigning values from $0$ to $K - 1$ where $K$ is number of unique features.\n",
    "<br>\n",
    "\n",
    "This encoder only works on a $n \\times 1$ array. Hence, only 1 column. If multiple columns are passed, it does list comparison.\n",
    "\n",
    "To use this encoder, import `LabelEncoder` from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cce0eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 1, 2, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Appling LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit_transform(np.ravel(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d85b6a",
   "metadata": {},
   "source": [
    "#### OrdinalEncoder\n",
    "\n",
    "An encoding method that works similar to label encoder, except that it can work with data having multiple columns. In this case, it does labeling from $0$ to $K - 1$ where $K$ is the number of unique features in the column. It does the same for every column.\n",
    "<br>\n",
    "\n",
    "To use this encoder, import `OrdinalEncoder` from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43b16a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0.],\n",
       "       [2., 2., 1.],\n",
       "       [0., 0., 2.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Appling OrdinalEncoder\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "oe.fit_transform(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca097f",
   "metadata": {},
   "source": [
    "### Discretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a014b4",
   "metadata": {},
   "source": [
    "Discretizer is a pre-processing tool that converts continuous data into discrete data in categorical, or ordinal forms by splitting the data into different parts or *bins*.\n",
    "<br>\n",
    "\n",
    "There is one majorly used discretizer:\n",
    "- KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6151723",
   "metadata": {},
   "source": [
    "#### KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae765a32",
   "metadata": {},
   "source": [
    "This discretizer takes in three parameters:\n",
    "- `n_bins`: number of bins\n",
    "- `strategy`: how bins are split\n",
    "- `encoder`: how to encode the bins. The encoders accepted are the same three encoders listed above.\n",
    "<br>\n",
    "\n",
    "`strategy` accepts three values:\n",
    "- `uniform`: split the bins with equal width\n",
    "- `qauntile`: split bins based on frequency. i.e. Number of values in each bin\n",
    "- `kmeans`: split bins based on k-means clustering value\n",
    "<br>\n",
    "\n",
    "To use this discretizer, import the `KBinsDiscretizer` class from `sklearn.preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7934b47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [5.],\n",
       "       [6.],\n",
       "       [6.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using KBinsDiscretizer\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "kbd = KBinsDiscretizer(n_bins = 7, strategy='uniform', encode='ordinal')\n",
    "kbd.fit_transform(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae997b",
   "metadata": {},
   "source": [
    "### Binirizer\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
