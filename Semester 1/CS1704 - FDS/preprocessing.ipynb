{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482d9ffe",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d69fbb",
   "metadata": {},
   "source": [
    "In the real world, the data we get is not always clean and ready to use. There may be missing values, features on different scales and non-number values.\n",
    "\n",
    "<br>\n",
    "\n",
    "Hence, there is a need to pre-process the data used to train the model efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9d781",
   "metadata": {},
   "source": [
    "## SKLearn Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4718fb",
   "metadata": {},
   "source": [
    "In Python, a library called `sklearn` (SciKit Learn) offers a wide variety of pre-processing transformers. It goes well with `numpy`, a library used for creating and processing numerical arrays.\n",
    "\n",
    "<br>\n",
    "\n",
    "It is important to use the same set of pre-processing for both the training data and the testing data. SKLearn offers a pipeline that makes it easy to chain multiple pre-processing transformers and apply them uniformly in the training data and the testing data. Here is a list of libraries provided by SKLearn for data pre-processing:\n",
    "- Data Cleaning\n",
    "- Feature Extraction\n",
    "- Feature Reduction\n",
    "- Feature Expansion\n",
    "\n",
    "<br>\n",
    "\n",
    "Upon getting the training data, it is important to explore the data to know what pre-processing is required. Typical problems include:\n",
    "- Missing values\n",
    "- Numerical values not being on the same scale\n",
    "- Categorical attributes not being represented in a numerical format\n",
    "- Too many features (must be reduced)\n",
    "- Extract features from non-numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b1360",
   "metadata": {},
   "source": [
    "### Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a77aa",
   "metadata": {},
   "source": [
    "Imputers are used to handle missing values in a dataset. The way imputers work is they rely on nearby data to the missing values and fill in the gap by doing some process with the nearby data.\n",
    "\n",
    "<br>\n",
    "\n",
    "Missing values include empty fields as well as non-numeric values (called NaN or Not a Number).\n",
    "\n",
    "<br>\n",
    "\n",
    "To use imputers, `impute` module from the `sklearn` library must be imported.\n",
    "\n",
    "<br>\n",
    "\n",
    "There are two main imputer APIs available in sklearn:\n",
    "- SimpleImputer\n",
    "- KNNImputer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad6571",
   "metadata": {},
   "source": [
    "#### SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b154f",
   "metadata": {},
   "source": [
    "This kind of imputer expects a 2D array, and fills in the missing values by checking the values in the column, and applies a strategy to compute a result with those values. The strategies available are:\n",
    "- mean: Computes the mean of other datas and fills in the result\n",
    "- median: Computes the median of other datas and fills in the result\n",
    "- most_frequent (Mode): Computes the mode of other datas and fills in the result\n",
    "- constant\n",
    "\n",
    "<br>\n",
    "\n",
    "If a column has multiple missing values, it fills in the values one by one. It ignores the other missing values in the column while processing.\n",
    "\n",
    "<br>\n",
    "\n",
    "To use this imputer, import `SimpleImputer` from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55dffcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Sample data with missing values\n",
    "X = [[1, 2], [np.nan, 3], [7, 6], [1, 4]]\n",
    "Y = [[np.nan, 2, 5], [6, np.nan, 4], [np.nan, 7, 6], [8, 2, np.nan]]\n",
    "Z = [[12], [2], [4], [-3], [6], [-5], [-100]]\n",
    "A = [[1], [2], [3], [9], [2], [3], [1]]\n",
    "B = [[1, \"human\", \"gamer\"], [2, \"robot\", \"machine\"], [0, \"bot\", \"speaker\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba2fb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 3.],\n",
       "       [7., 6.],\n",
       "       [1., 4.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filling in missing values with the mean strategy\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "imp.fit(X) #Learn the statistics from the data provided with variable X\n",
    "imp.transform(X) #Apply the learned stats to fill in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f5efb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [1., 3.],\n",
       "       [7., 6.],\n",
       "       [1., 4.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filling in missing values with the median strategy\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "imp.fit(X)\n",
    "imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1705ab4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [1., 3.],\n",
       "       [7., 6.],\n",
       "       [1., 4.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filling in missing values with the most_frequent strategy\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "imp.fit(X)\n",
    "imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4373242e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [0., 3.],\n",
       "       [7., 6.],\n",
       "       [1., 4.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filling in missing values with the constant strategy\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='constant')\n",
    "\n",
    "imp.fit(X)\n",
    "imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045ef81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.        , 2.        , 5.        ],\n",
       "       [6.        , 3.66666667, 4.        ],\n",
       "       [7.        , 7.        , 6.        ],\n",
       "       [8.        , 2.        , 5.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filling in missing values for a dataset with multiple nan values in a column\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(Y)\n",
    "imp.transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1024755",
   "metadata": {},
   "source": [
    "#### KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4749d7a1",
   "metadata": {},
   "source": [
    "This kind of imputer expects a 2D array data, and the missing values are filled by computing the mean of `n` nearest neighbours, where `n` is a value provided by the user.\n",
    "\n",
    "<br>\n",
    "\n",
    "To compute the nearest neighbours, it used euclidean distance, which is calcuulated using the formula:\n",
    "$$\\text {dist} = \\sqrt {\\text {weight} \\cdot \\text {(distance between the coordinates)}^2}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The weight is calculated by using the formula:\n",
    "$$\\text {weight} = \\frac{\\text {total number of data}}{\\text {number of numeric data}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The distance between the coordinates is the difference between the value on the row which contains the missing value and the value on the row which we consider. Both the values must be on the same column. If a value is nan or missing, we ignore its distance.\n",
    "\n",
    "<br>\n",
    "\n",
    "Consider the example: `X = [[1, 2, 3], [3, np.nan, 6], [np.nan, 4, 5]]`\n",
    "\n",
    "<br>\n",
    "\n",
    "The second row contains a nan value. So we take the second row (say `a`), and we also choose a row to compare with, say the first row (say `b`). \n",
    "\n",
    "<br>\n",
    "\n",
    "Now, `a = [1, 2, 3]` and `b = [3, np.nan, 6]`\n",
    "\n",
    "<br>\n",
    "\n",
    "The second column has a nan value, and it will be ignored. So the distance becomes : $$\\text {dist} = (1 - 3)^2 + (3 - 6)^2 = 13$$\n",
    "\n",
    "<br>\n",
    "\n",
    "There are three values in a row and only two columns don't have nan or missing values. So weight : $$\\text {weight} = \\frac {3}{2}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Now the euclidian distance becomes : $$\\sqrt {\\frac {3}{2} \\cdot 13} = \\sqrt {\\frac {39}{2}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "To use this imputer, import `KNNImputer` from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b943c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. , 4. ],\n",
       "       [3. , 4. , 3. ],\n",
       "       [5.5, 6. , 5. ],\n",
       "       [8. , 8. , 7. ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filling in the missing values using KNNImputer\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imp = KNNImputer(n_neighbors=2)\n",
    "X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n",
    "imp.fit(X)\n",
    "imp.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673719f6",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d295d7a6",
   "metadata": {},
   "source": [
    "Scalers are used to scale numbers so all of them are on the same scale. When values are on different scales, convergeence of iterative optimization procedures becomes slower.\n",
    "\n",
    "<br>\n",
    "\n",
    "To use scalers, the `preprocessing` module from `sklearn` must be imported.\n",
    "\n",
    "<br>\n",
    "\n",
    "There are three main Scaler APIs available in sklearn:\n",
    "- StandardScaler\n",
    "- MinMaxScaler\n",
    "- MaxAbsoluteScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f0caa",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee64101",
   "metadata": {},
   "source": [
    "Transforms the original feature `x` into `x'` using the formula: \n",
    "$$x \\text {'} = \\frac{x - \\mu}{\\sigma} \\text {, where } \\mu \\text { = Mean; } \\sigma \\text { = Standard Deviation}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "To use this scaler, import the `StandardScaler` class from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff8f8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.66107927],\n",
       "       [ 0.38562957],\n",
       "       [ 0.44071951],\n",
       "       [ 0.24790472],\n",
       "       [ 0.49580945],\n",
       "       [ 0.19281479],\n",
       "       [-2.42395731]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using StandardScaler to unify the scale\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(Z)\n",
    "ss.transform(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b25ee4",
   "metadata": {},
   "source": [
    "#### MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ed307",
   "metadata": {},
   "source": [
    "Transforms the original feature `x` into `x'` using the formula: \n",
    "$$x \\text {'} = \\frac{x - x_\\text {min}}{x_\\text {max} - x_\\text {min}}$$\n",
    "Where $x_\\text {min}$ is the minimum value in the matrix and $x_\\text {max}$ is the maximum value in the matrix. The scaled matrix has values from to 0 to 1, with both ends being present in any such matrix.\n",
    "\n",
    "<br>\n",
    "\n",
    "To use this scaler, import the `MinMaxScaler` class from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf6d7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030aabb6",
   "metadata": {},
   "source": [
    "## Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff8bef",
   "metadata": {},
   "source": [
    "This library in Python is uses `matplotlib` library underneath to plot graphs using provided data. It will be used to visualise random distributions.\n",
    "\n",
    "<br>\n",
    "\n",
    "To use it, import the `seaborn` library along with `pyplot` from the `matplotlib` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e35d77",
   "metadata": {},
   "source": [
    "#### Box Plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08afa578",
   "metadata": {},
   "source": [
    "#### Violin Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a719809",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Dataset with 3 categorical columns\u001b[39;00m\n\u001b[32m      5\u001b[39m X = np.array([\n\u001b[32m      6\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mRed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPetrol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mToyota\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      7\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mBlue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDiesel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHonda\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      8\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mGreen\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mElectric\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTesla\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      9\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mRed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDiesel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mToyota\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m encoder = \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m encoded = encoder.fit_transform(X)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCategories:\u001b[39m\u001b[33m\"\u001b[39m, encoder.categories_)\n",
      "\u001b[31mTypeError\u001b[39m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Dataset with 3 categorical columns\n",
    "X = np.array([\n",
    "    [\"Red\", \"Petrol\", \"Toyota\"],\n",
    "    [\"Blue\", \"Diesel\", \"Honda\"],\n",
    "    [\"Green\", \"Electric\", \"Tesla\"],\n",
    "    [\"Red\", \"Diesel\", \"Toyota\"]\n",
    "])\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded = encoder.fit_transform(X)\n",
    "\n",
    "print(\"Categories:\", encoder.categories_)\n",
    "print(\"Encoded:\\n\", encoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
